<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>burdock.models.dp_covariance API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>burdock.models.dp_covariance</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import numpy as np
import math


class DPcovariance():

    # Implementation is based off of https://github.com/privacytoolsproject/PSI-Library

    def __init__(self, n, cols, rng, global_eps, epsilon_dist=None, alpha=0.05):

        # TODO finish adding functionality for intercept
        intercept = False

        # The following variables are for different ways of setting up the epsilon value for DP covariance calculation
        # There is infrastructure for them, but we&#39;re currently choosing not to expose them.
        epsilon = None
        accuracy = None
        impute_rng = None
        accuracy_vals = None

        self.num_rows = n
        self.columns = cols

        self.intercept = intercept
        self.alpha = alpha

        self.rng = check_range(rng)
        self.sens = covariance_sensitivity(n, rng, intercept)

        if impute_rng is None:
            self.imputeRng = rng
        else:
            self.imputeRng = impute_rng

        if self.intercept:
            self.columns = [&#39;intercept&#39;] + self.columns
        else:
            self.columns = self.columns

        s = len(self.columns)
        output_length = (np.zeros((s, s))[np.tril_indices(s)]).size
        # Distribute epsilon across all covariances that will be calculated
        if epsilon is not None:
            self.epsilon = check_epsilon(epsilon, expected_length=output_length)
            self.globalEps = sum(self.epsilon)
        # Option 2: Enter global epsilon value and vector of percentages specifying how to split global
        # epsilon between covariance calculations.
        elif global_eps is not None and epsilon_dist is not None:
            self.globalEps = check_global_epsilon(global_eps)
            self.epsilonDist = check_epsilon_dist(epsilon_dist, output_length)
            self.epsilon = distribute_epsilon(self.globalEps, epsilon_dist=epsilon_dist)
            self.accuracyVals = laplace_get_accuracy(self.sens, self.epsilon, self.alpha)
        # Option 3: Only enter global epsilon, and have it be split evenly between covariance calculations.
        elif global_eps is not None:
            self.globalEps = check_global_epsilon(global_eps)
            self.epsilon = distribute_epsilon(self.globalEps, n_calcs=output_length)
            self.accuracyVals = laplace_get_accuracy(self.sens, self.epsilon, self.alpha)
        # Option 4: Enter an accuracy value instead of an epsilon, and calculate individual epsilons with this accuracy.
        elif accuracy is not None:
            self.accuracy = check_accuracy(accuracy)
            self.epsilon = laplace_get_epsilon(self.sens, self.accuracy, self.alpha)
            self.globalEps = sum(self.epsilon)
        # Option 5: Enter vector of accuracy values, and calculate ith epsilon value from ith accuracy value
        elif accuracy_vals is not None:
            self.accuracyVals = check_accuracy_vals(accuracy_vals, output_length)
            self.epsilon = laplace_get_epsilon(self.sens, self.accuracyVals, self.alpha)
            self.globalEps = sum(self.epsilon)

    def make_covar_symmetric(self, covar):
        &#34;&#34;&#34;
        Converts unique private covariances into symmetric matrix

        Args:
            covar (???): differentially privately release of elements in lower triangle of covariance matrix
        Returns:
            A symmetric differentially private covariance matrix (numpy array)
        &#34;&#34;&#34;
        n = len(self.columns)
        indices = np.triu_indices(n)
        m = np.zeros((n, n))
        m[indices] = covar
        m = m.T
        m = np.tril(m) + np.triu(m.T, 1)
        df = pd.DataFrame(m, columns=self.columns, index=self.columns)
        return df

    def release(self, data):
        new_data = censor_data(data[self.columns], self.rng)
        new_data = fill_missing(new_data, impute_rng=self.imputeRng)

        # TODO: add intercept functionality
        def covar(x, intercept=False):
            if intercept:
                pass  # TODO: Find python equivalent for the following R code: `x &lt; - cbind(1, x)`
            covariance = np.cov(x)
            return list(covariance[np.tril_indices(covariance.shape[0])])

        def q_lap_iter(p, mu=0, b=1):
            for i in range(len(p)):
                p[i] = q_lap(p[i], mu, b[i])
            return p

        def q_lap(elem, mu=0, b=1):
            if elem &lt; 0.5:
                return mu + b * np.log(2 * elem)
            else:
                return mu - b * np.log(2 - 2 * elem)

        def dp_noise(n, noise_scale):
            u = np.random.uniform(size=n)
            return q_lap_iter(u, b=noise_scale)

        true_val = covar(data.values.T, self.intercept)
        scale = self.sens / self.epsilon
        val = np.array(true_val) + dp_noise(n=len(true_val), noise_scale=scale)
        return list(val)

    # TODO: this implementation only works for one dependent variable right now
    def get_linear_regression(self, data, x_names, y_name, intercept=False):
        &#34;&#34;&#34;
        Takes in data, lists of feature names and target names, and whether or not
        we should calculate a y-intercept; and returns a DP linear regression model

        Args:
            data (Dataframe): the data that will be used to make the linear regression
            x_names (list): list of names of the features (i.e. independent variables) to use
            y_name (string): name of the target (i.e. dependent variable) to use
            intercept (boolean): true if the lin-reg equation should have a y-intercept, false if not
        Return:
            linear regression model (Dataframe) in the following format:
                Each independent variable gets its own row; there are two columns: &#39;Estimate&#39; and &#39;Std. Error&#39;.
                &#39;Estimate&#39; is the calculated coefficient for that row&#39;s corresponding independent variable,
                &#39;Std. Error&#39; is self evident.

                Here is an example return value given intercept=FALSE, independent variables &#39;Height&#39; and &#39;Volume&#39;
                and dependent variable &#39;Girth&#39;:

                           Estimate    Std. Error
                    Height -0.04548    0.02686
                    Volume  0.19518    0.01041
        &#34;&#34;&#34;
        covar_matrix = self.make_covar_symmetric(self.release(data))
        return cov_method_lin_reg(covar_matrix, self.num_rows, x_names, y_name, intercept)


def cov_method_lin_reg(release, num_rows, x_names, y_name, intercept=False):
    &#34;&#34;&#34;
        Takes in a differentially privately released covariance matrix, the number of rows in the
        original data, whether or not a y-intercept should be calculated, a list of
        feature names, and a target name; and returns a DP linear regression model

        Args:
            release (Dataframe): differentially privately released covariance matrix that will be used to make the linear regression
            num_rows (int): the number of rows in the original data
            x_names (list): list of names of the features (i.e. independent variables) to use
            y_name (string): name of the target (i.e. dependent variable) to use
            intercept (boolean): true if the lin-reg equation should have a y-intercept, false if not
        Returns:
            linear regression model (Dataframe) in the following format:
                Each independent variable gets its own row; there are two columns: &#39;Estimate&#39; and &#39;Std. Error&#39;.
                &#39;Estimate&#39; is the calculated coefficient for that row&#39;s corresponding independent variable,
                &#39;Std. Error&#39; is self evident.

                Here is an example return value given intercept=FALSE, independent variables &#39;Height&#39; and &#39;Volume&#39;
                and dependent variable &#39;Girth&#39;:

                           Estimate    Std. Error
                    Height -0.04548    0.02686
                    Volume  0.19518    0.01041
        &#34;&#34;&#34;
    eigenvals, _ = list(np.linalg.eig(release.values))
    if not all([ev != 0 for ev in eigenvals]):
        raise ValueError(&#34;Matrix is not invertible&#34;)
    elif not all([ev &gt; 0 for ev in eigenvals]):
        raise ValueError(&#34;Matrix is not positive definite&#34;)
    else:
        # Find locations corresponding to the given x &amp; y names
        loc_vec = [False] * release.shape[0]
        row_labels = release.index.values
        x_loc = []
        y_loc = None
        for index in range(len(row_labels)):
            if row_labels[index] in x_names:
                loc_vec[index] = True
                x_loc.append(index)
            if row_labels[index] == y_name:
                y_loc = index
        if x_loc is None or y_loc is None:
            raise ValueError(&#34;Names aren&#39;t found in the release&#34;)

        # Use a sweep to find the coefficient of the independent variable in
        # the linear regression corresponding to the covariance matrix
        sweep = amsweep(release.values / num_rows, np.array(loc_vec))
        coef = sweep[y_loc, x_loc]

        # Calculate the standard error
        submatrix = release.values[x_loc, :][:, x_loc]
        se = list(map(np.sqrt, sweep[y_loc, y_loc] * np.diag(np.linalg.inv(submatrix))))

        new_x_names = [release.index.values[x_loc[i]] for i in range(len(x_loc))]

        def round_5(elem):
            return round(elem, 5)

        # Round both values to account for floating point error, put in Series
        estimates = pd.Series(map(round_5, coef), index=new_x_names, name=&#39;Estimate&#39;)
        std_error = pd.Series(map(round_5, se), index=new_x_names, name=&#39;Std. Error&#39;)

        return pd.DataFrame([estimates, std_error]).transpose()


def check_accuracy_vals(accuracy_vals, expected_length):
    if len(accuracy_vals) != expected_length:
        raise ValueError(&#34;Epsilon parameter has improper length&#34;)
    else:
        for eps in accuracy_vals:
            if eps &lt;= 0:
                raise ValueError(&#34;Privacy parameter epsilon must be a value greater than zero&#34;)
    return accuracy_vals


def laplace_get_epsilon(sens, accuracy, alpha=.05):
    return np.log(1 / alpha) * (sens / accuracy)


def check_accuracy(accuracy):
    if accuracy &lt;= 0:
        raise ValueError(&#34;Privacy parameter epsilon must be a value greater than zero&#34;)
    return accuracy


def laplace_get_accuracy(sens, epsilon, alpha=.05):
    return np.log(1 / alpha) * (sens / epsilon)


def distribute_epsilon(global_eps, n_calcs=None, epsilon_dist=None):
    if epsilon_dist is None:
        eps = [global_eps / n_calcs for i in range(n_calcs)]
    else:
        eps = [eps * global_eps for eps in epsilon_dist]
    return eps


def check_epsilon_dist(epsilon_dist, expected_length):
    if len(epsilon_dist) != expected_length:
        raise ValueError(&#34;Epsilon parameter has improper length&#34;)
    else:
        for eps in epsilon_dist:
            if eps &lt;= 0:
                raise ValueError(&#34;All values in epsilonDist must be a value greater than zero&#34;)
        if sum(epsilon_dist) != 1.0:
            raise ValueError(&#34;All values in epsilonDist must sum to 1&#34;)
    return epsilon_dist


def check_epsilon(epsilon, expected_length):
    if len(epsilon) != expected_length:
        raise ValueError(&#34;Epsilon parameter has improper length&#34;)
    else:
        for eps in epsilon:
            if eps &lt;= 0:
                raise ValueError(&#34;(Privacy parameter epsilon must be a value greater than zero&#34;)
            elif eps &gt;= 3:
                raise ValueError(&#34;This is a higher global value than recommended for most cases&#34;)
    return epsilon


def check_global_epsilon(eps):
    if eps &lt;= 0:
        raise ValueError(&#34;(Privacy parameter epsilon must be a value greater than zero&#34;)
    elif eps &gt;= 3:
        raise ValueError(&#34;This is a higher global value than recommended for most cases&#34;)
    return eps


def covariance_sensitivity(n, rng, intercept):
    diffs = []
    for i in range(rng.shape[1]):
        diffs.append(rng[i][1] - rng[i][0])
    if intercept:
        diffs = [0] + diffs
    const = 2 / n
    sensitivity = []
    for i in range(len(diffs)):
        for j in range(i, len(diffs)):
            s = const * diffs[i] * diffs[j]
            sensitivity.append(s)
    return np.array(sensitivity)


def check_range(rng):
    rng.columns = list(range(rng.shape[1]))
    for col in range(rng.shape[1]):
        rng[col] = rng[col].sort_values()
    return rng


def fill_missing_1D(x, low, high):
    n_missing = x.isnull().sum()
    u = np.random.uniform(size=n_missing)

    def scale(v):
        return v * (high - low) + low

    u = list(map(scale, u))

    def replace_nan(v):
        if math.isnan(v):
            return u.pop()
        return v

    return x.apply(replace_nan)


def fill_missing(data, impute_rng):
    for i in range(data.shape[1]):
        data[i] = fill_missing_1D(data[i], impute_rng[i][0], impute_rng[i][1])
    return data


def censor(value, low, high):
    if value &lt; low:
        return low
    elif value &gt; high:
        return high
    else:
        return value


def censor_data_1D(x, l, h):
    def scale(v):
        return censor(v, l, h)

    return x.apply(scale)


def censor_data(data, rng):
    new_data = data

    new_data.columns = list(range(data.shape[1]))
    rng = check_range(rng)

    for i in range(data.shape[1]):
        data[i] = censor_data_1D(data[i], rng[i][0], rng[i][1])
    return data


def amsweep(g, m):
    &#34;&#34;&#34;
    Sweeps a covariance matrix to extract regression coefficients.

    Args:
        g (Numpy array): a numeric, symmetric covariance matrix divided by the number of observations in the data
        m (Numpy array): a logical vector of length equal to the number of rows in g
        in which the True values correspond to the x values in the matrix
        and the False values correspond to the y values in the matrix

    Return:
        a matrix with the coefficients from g
    &#34;&#34;&#34;
    # if m is a vector of all falses, then return g
    if np.array_equal(m, np.full(np.shape(m), False, dtype=bool)):
        return g
    else:
        p = np.shape(g)[0]  # number of rows of g (np.shape gives a tuple as (rows, cols), so we index [0])
        rowsm = sum(m)  # sum of logical vector &#34;m&#34; (m must be a (n,) shape np array)

        # if all values of m are True (thus making the sum equal to the length),
        # we take the inverse and then negate all the values
        if p == rowsm:
            h = np.linalg.inv(g)  # inverse of g
            h = np.negative(h)  # negate the sign of all elements
        else:
            k = np.where(m == True)[0]  # indices where m is True
            kcompl = np.where(m == False)[0]  # indices where m is False

            # separate the elements of g
            # make the type np.matrix so that dimensions are preserved correctly
            g11 = np.matrix(g[k, k])
            g12 = np.matrix(g[k, kcompl])
            g21 = np.transpose(g12)
            g22 = np.matrix(g[kcompl, kcompl])

            # use a try-except to get the inverse of g11
            try:
                h11a = np.linalg.inv(g11)  # try to get the regular inverse
            except:  # should have LinAlgError (not defined error)
                h11a = np.linalg.pinv(g11)
            h11 = np.negative(h11a)

            # matrix multiplication to get sections of h
            h12 = np.matmul(h11a, g12)
            h21 = np.transpose(h12)
            h22 = g22 - np.matmul(np.matmul(g21, h11a), g12)

            # combine sections of h
            hwo = np.concatenate((np.concatenate((h11, h12), axis=1), np.concatenate((h21, h22), axis=1)), axis=0)
            hwo = np.asarray(hwo)  # convert back to array (from matrix) to avoid weird indexing behavior
            xordering = np.concatenate((k, kcompl), axis=0)  # concatenate k and kcompl
            h = np.zeros((p, p))  # make a pxp array of zeros

            for i in range(p):  # traverse each element as defined by xordering
                for j in range(p):
                    h[xordering[i]][xordering[j]] = hwo[i][j]  # and replace it with the normal i, j element from hwo

        return h</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="burdock.models.dp_covariance.amsweep"><code class="name flex">
<span>def <span class="ident">amsweep</span></span>(<span>g, m)</span>
</code></dt>
<dd>
<section class="desc"><p>Sweeps a covariance matrix to extract regression coefficients.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>g</code></strong> :&ensp;<code>Numpy</code> <code>array</code></dt>
<dd>a numeric, symmetric covariance matrix divided by the number of observations in the data</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>Numpy</code> <code>array</code></dt>
<dd>a logical vector of length equal to the number of rows in g</dd>
</dl>
<p>in which the True values correspond to the x values in the matrix
and the False values correspond to the y values in the matrix</p>
<h2 id="return">Return</h2>
<p>a matrix with the coefficients from g</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def amsweep(g, m):
    &#34;&#34;&#34;
    Sweeps a covariance matrix to extract regression coefficients.

    Args:
        g (Numpy array): a numeric, symmetric covariance matrix divided by the number of observations in the data
        m (Numpy array): a logical vector of length equal to the number of rows in g
        in which the True values correspond to the x values in the matrix
        and the False values correspond to the y values in the matrix

    Return:
        a matrix with the coefficients from g
    &#34;&#34;&#34;
    # if m is a vector of all falses, then return g
    if np.array_equal(m, np.full(np.shape(m), False, dtype=bool)):
        return g
    else:
        p = np.shape(g)[0]  # number of rows of g (np.shape gives a tuple as (rows, cols), so we index [0])
        rowsm = sum(m)  # sum of logical vector &#34;m&#34; (m must be a (n,) shape np array)

        # if all values of m are True (thus making the sum equal to the length),
        # we take the inverse and then negate all the values
        if p == rowsm:
            h = np.linalg.inv(g)  # inverse of g
            h = np.negative(h)  # negate the sign of all elements
        else:
            k = np.where(m == True)[0]  # indices where m is True
            kcompl = np.where(m == False)[0]  # indices where m is False

            # separate the elements of g
            # make the type np.matrix so that dimensions are preserved correctly
            g11 = np.matrix(g[k, k])
            g12 = np.matrix(g[k, kcompl])
            g21 = np.transpose(g12)
            g22 = np.matrix(g[kcompl, kcompl])

            # use a try-except to get the inverse of g11
            try:
                h11a = np.linalg.inv(g11)  # try to get the regular inverse
            except:  # should have LinAlgError (not defined error)
                h11a = np.linalg.pinv(g11)
            h11 = np.negative(h11a)

            # matrix multiplication to get sections of h
            h12 = np.matmul(h11a, g12)
            h21 = np.transpose(h12)
            h22 = g22 - np.matmul(np.matmul(g21, h11a), g12)

            # combine sections of h
            hwo = np.concatenate((np.concatenate((h11, h12), axis=1), np.concatenate((h21, h22), axis=1)), axis=0)
            hwo = np.asarray(hwo)  # convert back to array (from matrix) to avoid weird indexing behavior
            xordering = np.concatenate((k, kcompl), axis=0)  # concatenate k and kcompl
            h = np.zeros((p, p))  # make a pxp array of zeros

            for i in range(p):  # traverse each element as defined by xordering
                for j in range(p):
                    h[xordering[i]][xordering[j]] = hwo[i][j]  # and replace it with the normal i, j element from hwo

        return h</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.censor"><code class="name flex">
<span>def <span class="ident">censor</span></span>(<span>value, low, high)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def censor(value, low, high):
    if value &lt; low:
        return low
    elif value &gt; high:
        return high
    else:
        return value</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.censor_data"><code class="name flex">
<span>def <span class="ident">censor_data</span></span>(<span>data, rng)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def censor_data(data, rng):
    new_data = data

    new_data.columns = list(range(data.shape[1]))
    rng = check_range(rng)

    for i in range(data.shape[1]):
        data[i] = censor_data_1D(data[i], rng[i][0], rng[i][1])
    return data</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.censor_data_1D"><code class="name flex">
<span>def <span class="ident">censor_data_1D</span></span>(<span>x, l, h)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def censor_data_1D(x, l, h):
    def scale(v):
        return censor(v, l, h)

    return x.apply(scale)</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.check_accuracy"><code class="name flex">
<span>def <span class="ident">check_accuracy</span></span>(<span>accuracy)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_accuracy(accuracy):
    if accuracy &lt;= 0:
        raise ValueError(&#34;Privacy parameter epsilon must be a value greater than zero&#34;)
    return accuracy</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.check_accuracy_vals"><code class="name flex">
<span>def <span class="ident">check_accuracy_vals</span></span>(<span>accuracy_vals, expected_length)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_accuracy_vals(accuracy_vals, expected_length):
    if len(accuracy_vals) != expected_length:
        raise ValueError(&#34;Epsilon parameter has improper length&#34;)
    else:
        for eps in accuracy_vals:
            if eps &lt;= 0:
                raise ValueError(&#34;Privacy parameter epsilon must be a value greater than zero&#34;)
    return accuracy_vals</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.check_epsilon"><code class="name flex">
<span>def <span class="ident">check_epsilon</span></span>(<span>epsilon, expected_length)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_epsilon(epsilon, expected_length):
    if len(epsilon) != expected_length:
        raise ValueError(&#34;Epsilon parameter has improper length&#34;)
    else:
        for eps in epsilon:
            if eps &lt;= 0:
                raise ValueError(&#34;(Privacy parameter epsilon must be a value greater than zero&#34;)
            elif eps &gt;= 3:
                raise ValueError(&#34;This is a higher global value than recommended for most cases&#34;)
    return epsilon</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.check_epsilon_dist"><code class="name flex">
<span>def <span class="ident">check_epsilon_dist</span></span>(<span>epsilon_dist, expected_length)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_epsilon_dist(epsilon_dist, expected_length):
    if len(epsilon_dist) != expected_length:
        raise ValueError(&#34;Epsilon parameter has improper length&#34;)
    else:
        for eps in epsilon_dist:
            if eps &lt;= 0:
                raise ValueError(&#34;All values in epsilonDist must be a value greater than zero&#34;)
        if sum(epsilon_dist) != 1.0:
            raise ValueError(&#34;All values in epsilonDist must sum to 1&#34;)
    return epsilon_dist</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.check_global_epsilon"><code class="name flex">
<span>def <span class="ident">check_global_epsilon</span></span>(<span>eps)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_global_epsilon(eps):
    if eps &lt;= 0:
        raise ValueError(&#34;(Privacy parameter epsilon must be a value greater than zero&#34;)
    elif eps &gt;= 3:
        raise ValueError(&#34;This is a higher global value than recommended for most cases&#34;)
    return eps</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.check_range"><code class="name flex">
<span>def <span class="ident">check_range</span></span>(<span>rng)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_range(rng):
    rng.columns = list(range(rng.shape[1]))
    for col in range(rng.shape[1]):
        rng[col] = rng[col].sort_values()
    return rng</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.cov_method_lin_reg"><code class="name flex">
<span>def <span class="ident">cov_method_lin_reg</span></span>(<span>release, num_rows, x_names, y_name, intercept=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes in a differentially privately released covariance matrix, the number of rows in the
original data, whether or not a y-intercept should be calculated, a list of
feature names, and a target name; and returns a DP linear regression model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>release</code></strong> :&ensp;<code>Dataframe</code></dt>
<dd>differentially privately released covariance matrix that will be used to make the linear regression</dd>
<dt><strong><code>num_rows</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of rows in the original data</dd>
<dt><strong><code>x_names</code></strong> :&ensp;<code>list</code></dt>
<dd>list of names of the features (i.e. independent variables) to use</dd>
<dt><strong><code>y_name</code></strong> :&ensp;<code>string</code></dt>
<dd>name of the target (i.e. dependent variable) to use</dd>
<dt><strong><code>intercept</code></strong> :&ensp;<code>boolean</code></dt>
<dd>true if the lin-reg equation should have a y-intercept, false if not</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>linear</code> <code>regression</code> <code>model</code> (<code>Dataframe</code>) <code>in</code> <code>the</code> <code>following</code> <code>format</code>:</dt>
<dd>
<p>Each independent variable gets its own row; there are two columns: 'Estimate' and 'Std. Error'.
'Estimate' is the calculated coefficient for that row's corresponding independent variable,
'Std. Error' is self evident.</p>
<p>Here is an example return value given intercept=FALSE, independent variables 'Height' and 'Volume'
and dependent variable 'Girth':</p>
<pre><code>       Estimate    Std. Error
Height -0.04548    0.02686
Volume  0.19518    0.01041
</code></pre>
</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cov_method_lin_reg(release, num_rows, x_names, y_name, intercept=False):
    &#34;&#34;&#34;
        Takes in a differentially privately released covariance matrix, the number of rows in the
        original data, whether or not a y-intercept should be calculated, a list of
        feature names, and a target name; and returns a DP linear regression model

        Args:
            release (Dataframe): differentially privately released covariance matrix that will be used to make the linear regression
            num_rows (int): the number of rows in the original data
            x_names (list): list of names of the features (i.e. independent variables) to use
            y_name (string): name of the target (i.e. dependent variable) to use
            intercept (boolean): true if the lin-reg equation should have a y-intercept, false if not
        Returns:
            linear regression model (Dataframe) in the following format:
                Each independent variable gets its own row; there are two columns: &#39;Estimate&#39; and &#39;Std. Error&#39;.
                &#39;Estimate&#39; is the calculated coefficient for that row&#39;s corresponding independent variable,
                &#39;Std. Error&#39; is self evident.

                Here is an example return value given intercept=FALSE, independent variables &#39;Height&#39; and &#39;Volume&#39;
                and dependent variable &#39;Girth&#39;:

                           Estimate    Std. Error
                    Height -0.04548    0.02686
                    Volume  0.19518    0.01041
        &#34;&#34;&#34;
    eigenvals, _ = list(np.linalg.eig(release.values))
    if not all([ev != 0 for ev in eigenvals]):
        raise ValueError(&#34;Matrix is not invertible&#34;)
    elif not all([ev &gt; 0 for ev in eigenvals]):
        raise ValueError(&#34;Matrix is not positive definite&#34;)
    else:
        # Find locations corresponding to the given x &amp; y names
        loc_vec = [False] * release.shape[0]
        row_labels = release.index.values
        x_loc = []
        y_loc = None
        for index in range(len(row_labels)):
            if row_labels[index] in x_names:
                loc_vec[index] = True
                x_loc.append(index)
            if row_labels[index] == y_name:
                y_loc = index
        if x_loc is None or y_loc is None:
            raise ValueError(&#34;Names aren&#39;t found in the release&#34;)

        # Use a sweep to find the coefficient of the independent variable in
        # the linear regression corresponding to the covariance matrix
        sweep = amsweep(release.values / num_rows, np.array(loc_vec))
        coef = sweep[y_loc, x_loc]

        # Calculate the standard error
        submatrix = release.values[x_loc, :][:, x_loc]
        se = list(map(np.sqrt, sweep[y_loc, y_loc] * np.diag(np.linalg.inv(submatrix))))

        new_x_names = [release.index.values[x_loc[i]] for i in range(len(x_loc))]

        def round_5(elem):
            return round(elem, 5)

        # Round both values to account for floating point error, put in Series
        estimates = pd.Series(map(round_5, coef), index=new_x_names, name=&#39;Estimate&#39;)
        std_error = pd.Series(map(round_5, se), index=new_x_names, name=&#39;Std. Error&#39;)

        return pd.DataFrame([estimates, std_error]).transpose()</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.covariance_sensitivity"><code class="name flex">
<span>def <span class="ident">covariance_sensitivity</span></span>(<span>n, rng, intercept)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covariance_sensitivity(n, rng, intercept):
    diffs = []
    for i in range(rng.shape[1]):
        diffs.append(rng[i][1] - rng[i][0])
    if intercept:
        diffs = [0] + diffs
    const = 2 / n
    sensitivity = []
    for i in range(len(diffs)):
        for j in range(i, len(diffs)):
            s = const * diffs[i] * diffs[j]
            sensitivity.append(s)
    return np.array(sensitivity)</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.distribute_epsilon"><code class="name flex">
<span>def <span class="ident">distribute_epsilon</span></span>(<span>global_eps, n_calcs=None, epsilon_dist=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distribute_epsilon(global_eps, n_calcs=None, epsilon_dist=None):
    if epsilon_dist is None:
        eps = [global_eps / n_calcs for i in range(n_calcs)]
    else:
        eps = [eps * global_eps for eps in epsilon_dist]
    return eps</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.fill_missing"><code class="name flex">
<span>def <span class="ident">fill_missing</span></span>(<span>data, impute_rng)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_missing(data, impute_rng):
    for i in range(data.shape[1]):
        data[i] = fill_missing_1D(data[i], impute_rng[i][0], impute_rng[i][1])
    return data</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.fill_missing_1D"><code class="name flex">
<span>def <span class="ident">fill_missing_1D</span></span>(<span>x, low, high)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_missing_1D(x, low, high):
    n_missing = x.isnull().sum()
    u = np.random.uniform(size=n_missing)

    def scale(v):
        return v * (high - low) + low

    u = list(map(scale, u))

    def replace_nan(v):
        if math.isnan(v):
            return u.pop()
        return v

    return x.apply(replace_nan)</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.laplace_get_accuracy"><code class="name flex">
<span>def <span class="ident">laplace_get_accuracy</span></span>(<span>sens, epsilon, alpha=0.05)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def laplace_get_accuracy(sens, epsilon, alpha=.05):
    return np.log(1 / alpha) * (sens / epsilon)</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.laplace_get_epsilon"><code class="name flex">
<span>def <span class="ident">laplace_get_epsilon</span></span>(<span>sens, accuracy, alpha=0.05)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def laplace_get_epsilon(sens, accuracy, alpha=.05):
    return np.log(1 / alpha) * (sens / accuracy)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="burdock.models.dp_covariance.DPcovariance"><code class="flex name class">
<span>class <span class="ident">DPcovariance</span></span>
<span>(</span><span>n, cols, rng, global_eps, epsilon_dist=None, alpha=0.05)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DPcovariance():

    # Implementation is based off of https://github.com/privacytoolsproject/PSI-Library

    def __init__(self, n, cols, rng, global_eps, epsilon_dist=None, alpha=0.05):

        # TODO finish adding functionality for intercept
        intercept = False

        # The following variables are for different ways of setting up the epsilon value for DP covariance calculation
        # There is infrastructure for them, but we&#39;re currently choosing not to expose them.
        epsilon = None
        accuracy = None
        impute_rng = None
        accuracy_vals = None

        self.num_rows = n
        self.columns = cols

        self.intercept = intercept
        self.alpha = alpha

        self.rng = check_range(rng)
        self.sens = covariance_sensitivity(n, rng, intercept)

        if impute_rng is None:
            self.imputeRng = rng
        else:
            self.imputeRng = impute_rng

        if self.intercept:
            self.columns = [&#39;intercept&#39;] + self.columns
        else:
            self.columns = self.columns

        s = len(self.columns)
        output_length = (np.zeros((s, s))[np.tril_indices(s)]).size
        # Distribute epsilon across all covariances that will be calculated
        if epsilon is not None:
            self.epsilon = check_epsilon(epsilon, expected_length=output_length)
            self.globalEps = sum(self.epsilon)
        # Option 2: Enter global epsilon value and vector of percentages specifying how to split global
        # epsilon between covariance calculations.
        elif global_eps is not None and epsilon_dist is not None:
            self.globalEps = check_global_epsilon(global_eps)
            self.epsilonDist = check_epsilon_dist(epsilon_dist, output_length)
            self.epsilon = distribute_epsilon(self.globalEps, epsilon_dist=epsilon_dist)
            self.accuracyVals = laplace_get_accuracy(self.sens, self.epsilon, self.alpha)
        # Option 3: Only enter global epsilon, and have it be split evenly between covariance calculations.
        elif global_eps is not None:
            self.globalEps = check_global_epsilon(global_eps)
            self.epsilon = distribute_epsilon(self.globalEps, n_calcs=output_length)
            self.accuracyVals = laplace_get_accuracy(self.sens, self.epsilon, self.alpha)
        # Option 4: Enter an accuracy value instead of an epsilon, and calculate individual epsilons with this accuracy.
        elif accuracy is not None:
            self.accuracy = check_accuracy(accuracy)
            self.epsilon = laplace_get_epsilon(self.sens, self.accuracy, self.alpha)
            self.globalEps = sum(self.epsilon)
        # Option 5: Enter vector of accuracy values, and calculate ith epsilon value from ith accuracy value
        elif accuracy_vals is not None:
            self.accuracyVals = check_accuracy_vals(accuracy_vals, output_length)
            self.epsilon = laplace_get_epsilon(self.sens, self.accuracyVals, self.alpha)
            self.globalEps = sum(self.epsilon)

    def make_covar_symmetric(self, covar):
        &#34;&#34;&#34;
        Converts unique private covariances into symmetric matrix

        Args:
            covar (???): differentially privately release of elements in lower triangle of covariance matrix
        Returns:
            A symmetric differentially private covariance matrix (numpy array)
        &#34;&#34;&#34;
        n = len(self.columns)
        indices = np.triu_indices(n)
        m = np.zeros((n, n))
        m[indices] = covar
        m = m.T
        m = np.tril(m) + np.triu(m.T, 1)
        df = pd.DataFrame(m, columns=self.columns, index=self.columns)
        return df

    def release(self, data):
        new_data = censor_data(data[self.columns], self.rng)
        new_data = fill_missing(new_data, impute_rng=self.imputeRng)

        # TODO: add intercept functionality
        def covar(x, intercept=False):
            if intercept:
                pass  # TODO: Find python equivalent for the following R code: `x &lt; - cbind(1, x)`
            covariance = np.cov(x)
            return list(covariance[np.tril_indices(covariance.shape[0])])

        def q_lap_iter(p, mu=0, b=1):
            for i in range(len(p)):
                p[i] = q_lap(p[i], mu, b[i])
            return p

        def q_lap(elem, mu=0, b=1):
            if elem &lt; 0.5:
                return mu + b * np.log(2 * elem)
            else:
                return mu - b * np.log(2 - 2 * elem)

        def dp_noise(n, noise_scale):
            u = np.random.uniform(size=n)
            return q_lap_iter(u, b=noise_scale)

        true_val = covar(data.values.T, self.intercept)
        scale = self.sens / self.epsilon
        val = np.array(true_val) + dp_noise(n=len(true_val), noise_scale=scale)
        return list(val)

    # TODO: this implementation only works for one dependent variable right now
    def get_linear_regression(self, data, x_names, y_name, intercept=False):
        &#34;&#34;&#34;
        Takes in data, lists of feature names and target names, and whether or not
        we should calculate a y-intercept; and returns a DP linear regression model

        Args:
            data (Dataframe): the data that will be used to make the linear regression
            x_names (list): list of names of the features (i.e. independent variables) to use
            y_name (string): name of the target (i.e. dependent variable) to use
            intercept (boolean): true if the lin-reg equation should have a y-intercept, false if not
        Return:
            linear regression model (Dataframe) in the following format:
                Each independent variable gets its own row; there are two columns: &#39;Estimate&#39; and &#39;Std. Error&#39;.
                &#39;Estimate&#39; is the calculated coefficient for that row&#39;s corresponding independent variable,
                &#39;Std. Error&#39; is self evident.

                Here is an example return value given intercept=FALSE, independent variables &#39;Height&#39; and &#39;Volume&#39;
                and dependent variable &#39;Girth&#39;:

                           Estimate    Std. Error
                    Height -0.04548    0.02686
                    Volume  0.19518    0.01041
        &#34;&#34;&#34;
        covar_matrix = self.make_covar_symmetric(self.release(data))
        return cov_method_lin_reg(covar_matrix, self.num_rows, x_names, y_name, intercept)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="burdock.models.dp_covariance.DPcovariance.get_linear_regression"><code class="name flex">
<span>def <span class="ident">get_linear_regression</span></span>(<span>self, data, x_names, y_name, intercept=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes in data, lists of feature names and target names, and whether or not
we should calculate a y-intercept; and returns a DP linear regression model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Dataframe</code></dt>
<dd>the data that will be used to make the linear regression</dd>
<dt><strong><code>x_names</code></strong> :&ensp;<code>list</code></dt>
<dd>list of names of the features (i.e. independent variables) to use</dd>
<dt><strong><code>y_name</code></strong> :&ensp;<code>string</code></dt>
<dd>name of the target (i.e. dependent variable) to use</dd>
<dt><strong><code>intercept</code></strong> :&ensp;<code>boolean</code></dt>
<dd>true if the lin-reg equation should have a y-intercept, false if not</dd>
</dl>
<h2 id="return">Return</h2>
<p>linear regression model (Dataframe) in the following format:
Each independent variable gets its own row; there are two columns: 'Estimate' and 'Std. Error'.
'Estimate' is the calculated coefficient for that row's corresponding independent variable,
'Std. Error' is self evident.</p>
<pre><code>Here is an example return value given intercept=FALSE, independent variables 'Height' and 'Volume'
and dependent variable 'Girth':

           Estimate    Std. Error
    Height -0.04548    0.02686
    Volume  0.19518    0.01041
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_linear_regression(self, data, x_names, y_name, intercept=False):
    &#34;&#34;&#34;
    Takes in data, lists of feature names and target names, and whether or not
    we should calculate a y-intercept; and returns a DP linear regression model

    Args:
        data (Dataframe): the data that will be used to make the linear regression
        x_names (list): list of names of the features (i.e. independent variables) to use
        y_name (string): name of the target (i.e. dependent variable) to use
        intercept (boolean): true if the lin-reg equation should have a y-intercept, false if not
    Return:
        linear regression model (Dataframe) in the following format:
            Each independent variable gets its own row; there are two columns: &#39;Estimate&#39; and &#39;Std. Error&#39;.
            &#39;Estimate&#39; is the calculated coefficient for that row&#39;s corresponding independent variable,
            &#39;Std. Error&#39; is self evident.

            Here is an example return value given intercept=FALSE, independent variables &#39;Height&#39; and &#39;Volume&#39;
            and dependent variable &#39;Girth&#39;:

                       Estimate    Std. Error
                Height -0.04548    0.02686
                Volume  0.19518    0.01041
    &#34;&#34;&#34;
    covar_matrix = self.make_covar_symmetric(self.release(data))
    return cov_method_lin_reg(covar_matrix, self.num_rows, x_names, y_name, intercept)</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.DPcovariance.make_covar_symmetric"><code class="name flex">
<span>def <span class="ident">make_covar_symmetric</span></span>(<span>self, covar)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts unique private covariances into symmetric matrix</p>
<h2 id="args">Args</h2>
<p>covar (???): differentially privately release of elements in lower triangle of covariance matrix</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>A</code> <code>symmetric</code> <code>differentially</code> <code>private</code> <code>covariance</code> <code>matrix</code> (<code>numpy</code> <code>array</code>)</dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_covar_symmetric(self, covar):
    &#34;&#34;&#34;
    Converts unique private covariances into symmetric matrix

    Args:
        covar (???): differentially privately release of elements in lower triangle of covariance matrix
    Returns:
        A symmetric differentially private covariance matrix (numpy array)
    &#34;&#34;&#34;
    n = len(self.columns)
    indices = np.triu_indices(n)
    m = np.zeros((n, n))
    m[indices] = covar
    m = m.T
    m = np.tril(m) + np.triu(m.T, 1)
    df = pd.DataFrame(m, columns=self.columns, index=self.columns)
    return df</code></pre>
</details>
</dd>
<dt id="burdock.models.dp_covariance.DPcovariance.release"><code class="name flex">
<span>def <span class="ident">release</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def release(self, data):
    new_data = censor_data(data[self.columns], self.rng)
    new_data = fill_missing(new_data, impute_rng=self.imputeRng)

    # TODO: add intercept functionality
    def covar(x, intercept=False):
        if intercept:
            pass  # TODO: Find python equivalent for the following R code: `x &lt; - cbind(1, x)`
        covariance = np.cov(x)
        return list(covariance[np.tril_indices(covariance.shape[0])])

    def q_lap_iter(p, mu=0, b=1):
        for i in range(len(p)):
            p[i] = q_lap(p[i], mu, b[i])
        return p

    def q_lap(elem, mu=0, b=1):
        if elem &lt; 0.5:
            return mu + b * np.log(2 * elem)
        else:
            return mu - b * np.log(2 - 2 * elem)

    def dp_noise(n, noise_scale):
        u = np.random.uniform(size=n)
        return q_lap_iter(u, b=noise_scale)

    true_val = covar(data.values.T, self.intercept)
    scale = self.sens / self.epsilon
    val = np.array(true_val) + dp_noise(n=len(true_val), noise_scale=scale)
    return list(val)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="burdock.models" href="index.html">burdock.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="burdock.models.dp_covariance.amsweep" href="#burdock.models.dp_covariance.amsweep">amsweep</a></code></li>
<li><code><a title="burdock.models.dp_covariance.censor" href="#burdock.models.dp_covariance.censor">censor</a></code></li>
<li><code><a title="burdock.models.dp_covariance.censor_data" href="#burdock.models.dp_covariance.censor_data">censor_data</a></code></li>
<li><code><a title="burdock.models.dp_covariance.censor_data_1D" href="#burdock.models.dp_covariance.censor_data_1D">censor_data_1D</a></code></li>
<li><code><a title="burdock.models.dp_covariance.check_accuracy" href="#burdock.models.dp_covariance.check_accuracy">check_accuracy</a></code></li>
<li><code><a title="burdock.models.dp_covariance.check_accuracy_vals" href="#burdock.models.dp_covariance.check_accuracy_vals">check_accuracy_vals</a></code></li>
<li><code><a title="burdock.models.dp_covariance.check_epsilon" href="#burdock.models.dp_covariance.check_epsilon">check_epsilon</a></code></li>
<li><code><a title="burdock.models.dp_covariance.check_epsilon_dist" href="#burdock.models.dp_covariance.check_epsilon_dist">check_epsilon_dist</a></code></li>
<li><code><a title="burdock.models.dp_covariance.check_global_epsilon" href="#burdock.models.dp_covariance.check_global_epsilon">check_global_epsilon</a></code></li>
<li><code><a title="burdock.models.dp_covariance.check_range" href="#burdock.models.dp_covariance.check_range">check_range</a></code></li>
<li><code><a title="burdock.models.dp_covariance.cov_method_lin_reg" href="#burdock.models.dp_covariance.cov_method_lin_reg">cov_method_lin_reg</a></code></li>
<li><code><a title="burdock.models.dp_covariance.covariance_sensitivity" href="#burdock.models.dp_covariance.covariance_sensitivity">covariance_sensitivity</a></code></li>
<li><code><a title="burdock.models.dp_covariance.distribute_epsilon" href="#burdock.models.dp_covariance.distribute_epsilon">distribute_epsilon</a></code></li>
<li><code><a title="burdock.models.dp_covariance.fill_missing" href="#burdock.models.dp_covariance.fill_missing">fill_missing</a></code></li>
<li><code><a title="burdock.models.dp_covariance.fill_missing_1D" href="#burdock.models.dp_covariance.fill_missing_1D">fill_missing_1D</a></code></li>
<li><code><a title="burdock.models.dp_covariance.laplace_get_accuracy" href="#burdock.models.dp_covariance.laplace_get_accuracy">laplace_get_accuracy</a></code></li>
<li><code><a title="burdock.models.dp_covariance.laplace_get_epsilon" href="#burdock.models.dp_covariance.laplace_get_epsilon">laplace_get_epsilon</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="burdock.models.dp_covariance.DPcovariance" href="#burdock.models.dp_covariance.DPcovariance">DPcovariance</a></code></h4>
<ul class="">
<li><code><a title="burdock.models.dp_covariance.DPcovariance.get_linear_regression" href="#burdock.models.dp_covariance.DPcovariance.get_linear_regression">get_linear_regression</a></code></li>
<li><code><a title="burdock.models.dp_covariance.DPcovariance.make_covar_symmetric" href="#burdock.models.dp_covariance.DPcovariance.make_covar_symmetric">make_covar_symmetric</a></code></li>
<li><code><a title="burdock.models.dp_covariance.DPcovariance.release" href="#burdock.models.dp_covariance.DPcovariance.release">release</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>